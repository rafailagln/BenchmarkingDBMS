\section{Discussion}
\label{sec:discussion}

In this section, we delve into the analysis of the Evaluation outcomes and endeavor to comprehend and explicate them. Firstly, as shown in Figures~\ref{fig:311Cache}, \ref{fig:WeblogsCache},
\ref{fig:Zillow1Cache}, \ref{fig:Zillow2Cache}, column-based databases such as MonetDB and 
Vertica, outperform compared to the row-based PostgreSQL and the NoSQL 
database MongoDB. This is reasonable, as a result of the Vectorization of column-based 
databases, since they perform fewer function calls. More specifically, fewer calls are triggered 
to Python UDFs, which are slower than the embedded functions of the databases because they 
are executed on the PythonEngine.

\subsection{General observations}
Specifically, \textbf{Vectorization} in columnar databases encompasses the handling of a 
multitude of data elements in a solitary transaction, rather than processing each individually.
In a column-based database, data is stored in columns rather than rows, so vectorized 
operations can take advantage of this layout to perform faster processing on large data 
sets. For example, in our queries, UDFs needed to be applied on each record, 
expecting as an argument one field. If data were processed horizontally (row-based DBs), 
for every single field, different UDF should be applied, and it would be slower for the system
to complete the query in every record (for each row). Conversely, vectorized operation 
processed the entire column of fields in a single operation, rather than applying UDF on each 
record individually. 
This can result in significant performance improvements, because the 
processor can work on large amounts of data in parallel, rather than having to switch between 
individual operations. Vectorization is a key feature of column-based databases and is one of 
the reasons why they can provide fast and efficient processing of big data sets.

For most of the benchmarks the query execution times \textbf{scale linearly} when the input data increases, as seen
more obviously from Figure~\ref{fig:311ParallelismPostgreSQL} to Figure~\ref{fig:Zillow2ParallelismMongo}.
However, query execution time depends, also, on the complexity of the 
query, hardware, database management system, and database schema. 
On the other hand, as aforementioned, column-based and more specifically, Verica DB 
is not compliant to this observation. In scope Zillow1 query, which is consisted
of many nested calls, as data increases, Vertica DB handles the query more efficiently 
(see Figure~~\ref{fig:Zillow1Cache} and ~\ref{fig:Zillow1ParallelismVertica}). 

% This happens because the amount of work required to retrieve and process the data in a 
% query increases as the size of the data increases. For example, if a query requires 
% scanning through all of the data in a table, then as the size of the table doubles, 
% the amount of time required to scan the table will also double. Similarly, if a query 
% involves many aggregations, then as the 
% number of records increases, the amount of time required to perform the calculation or 
% processing will also increase.

\subsection{Observation on cold/warm caches}
We conducted our experiments with two different cache states, as discussed in 
Section~\ref{subsec:cache}. As it is showcased in Figures~\ref{fig:311Cache}, \ref{fig:WeblogsCache},
\ref{fig:Zillow1Cache}, \ref{fig:Zillow2Cache}, the performance has no significant 
discrepancies between cold and warm environments. 
Size of the data used, mentioned in Section~\ref{subsec:cache}, illustrates the meager
need for I/O, as the query execution also results in small output to be written in disk.
One worth mentioning point, can be seen on Zillow1 query in PostgresSQl, where a slight increase 
in performance in the warm cache environment is observed. 

Another reason of not seeing any significant difference can be the disk used, since 
cache memory management relies on it. To support this assumption, we conducted another experiment.
We installed PostgreSQL on an HDD disk and ran the cache memory experiments for Zillow2 query again for datasets 8 to 128 times larger than the original data~\footnote{The experiments have been conducted for sizes from 1 
to 128 times larger than the original data, but for better presentation these were selected so 
that the diagram is more understandable and legible.}. The Figure~\ref{fig:PostgresHDDCache}, for 
each dataset shows the execution times for each iteration. As can easily be noticed, the 
initial iteration for each dataset is with a clean cache, using the command~\ref{lst:paralDF} 
described in the Cache Section~\ref{subsec:cache}. 
It is showcased that the first iteration is always slower as the database needs to 
communicate for the first time with the slower HDD disk. After this iteration, the times 
stabilize as the data is now cached in the RAM and doesn't need to communicate with the disk. 
This phenomenon was not observed when using an SSD disk as its speeds are high enough not to 
show the difference.

\subsection{Observations of the parallelism}
The second factor the DBMSs have been with is parallelism. Executing a query in parallel, 
results in a significant improvement in time. The ideal improvement factor is equal to the 
number of extra processes/threads given to the system, that is, the more the number 
of processes/threads, the less the execution time. Of course, this is the 
theoretical maximum speedup and can never be achieved because there is the communication 
time between the processes and also parts of the queries that cannot be parallelized. 
The latter is also known as Amdahl's 
law~\footnote{\url{http://www.cslab.ntua.gr/courses/pps/files/fall2014/pps-notes.pdf}},
\footnote{\url{https://en.wikipedia.org/wiki/Amdahl\%27s\_law}}. 
Amdahl's law states that if 
$f$ is the proportion of a program that can be made parallel (i.e. benefit from parallelization), 
and $1-f$ is the proportion that cannot be paralleled (remains serial), then the maximum 
speedup that can be achieved by using $p$ processors is: 
\begin{equation}
    S_{max} = \frac{1}{\left ( 1-f \right ) + \frac{f}{p}}
\end{equation}
Although simple in its essence, Amdahl's law provides the initial and most significant 
guide to parallelization and in general to code optimization: any optimization approach 
should focus on the part of the algorithm that dominates the execution time.

\textbf{PostgreSQL}, as seen in Figures~\ref{fig:311ParallelismPostgreSQL},\ref{fig:WeblogsParallelismPostgreSQL},\ref{fig:Zillow1ParallelismPostgreSQL},\ref{fig:Zillow2ParallelismPostgreSQL}, presents no performance improvement 
when running on multiple processes.
Since Python uses parallelism at the process level too, 
improvements in performance would be expected. Observing the execution of queries 
in PostgreSQL (using the \emph{EXPLAIN} and \emph{ANALYZE} commands), the database's 
query planner did not choose to add more workers (as they are called in the PostgreSQL language) 
during query execution, effectively ignoring the settings in the configuration file. 
We believe this is because the planner has a better overview of the system and deemed 
that adding more workers would not benefit system performance. 
Additionally, since data consists of multiple iterations of the 
original data, they have a relatively uniform distribution and are relatively predictable, 
so that the query planner can keep statistics that help him make decisions. 


\textbf{MonetDB}, similarly, as seen in Figures~\ref{fig:311ParallelismMonet},
\ref{fig:WeblogsParallelismMonet},\ref{fig:Zillow1ParallelismMonet},
\ref{fig:Zillow2ParallelismMonet}, does not present significant scale, 
by adding extra threads. 
As it is mentioned in Section~\ref{par:MonetFBPar}, Python does not use threads for 
parallelism but processes, so we could say that this is a reason no difference appears. 
Also, MonetDB has its own form of parallelism in which a pipeline is created internally
to pass data to the next processing stage more quickly. Disabling this feature (setting the 
value to $sequential\_pipe$) and again no significant difference seen in the performance of 
the queries. The three main reasons for not seeing any difference on performance, 
have already showcased and are namely, hardware, data size and distribution 
and query complexity. Finally, we consider that the improvements observed in 
Figure~\ref{fig:WeblogsParallelismMonet} are some anomaly of the system.

\textbf{Vertica}, especially, in the Zillow2 query 
(Figure~\ref{fig:Zillow2ParallelismVertica}), which consists of many aggregated functions 
(SUM, COUNT, and GROUP BY), adding one extra core is quite significant.
Hence, aggregated functions and data can be exploited to a large extent in parallelism. 
The former in SQL are helped by parallelization because they are designed to perform operations on a 
large amount of data. When these operations are executed in parallel, they can be completed 
much faster than if they were executed sequentially. Parallel processing divides the data into 
smaller parts, allowing each part to be processed simultaneously. Additionally, in Weblogs query 
(Figure~\ref{fig:WeblogsParallelismVertica}) can be noticed a slight improvement 
in performance. This might be due to the complexity of the query, as it contains a JOIN 
operation that can be benefited from parallelism. This is because parallel processing 
allows the join operation to be divided into smaller, independent tasks, which can be 
executed simultaneously.

\subsection{MongoDB performance}
Regarding \textbf{MongoDB}, although most the database's functionalities have not be utilized throughout the 
experiments, there have been noticed some interesting observations in both cache and
parallelization cases. 
Firstly, having implemented the Zillow1 query using explicitly 
python filtering, has been proved much faster, but we chose \emph{pandas} option
for compliance reasons. Moreover, pandas gave us the choice of the 
implementation of kernel parallelization~\footnote{Both approaches can be found in our Github repository
under mongo/src/folder, as well, as the measurements of each approach.}. 

In addition, as shown in Figures~\ref{fig:Zillow1ParallelismMongo}, 
\ref{fig:Zillow2ParallelismMongo}, \ref{fig:311ParallelismMongo}, 
MongoDB not only merrits from parallel execution, but also it showcases 
linear speedups while the number of cores increases.
For example, Figure~\ref{fig:WeblogsParallelismMongo} showcases that running 
the Zillow1 on 2 cores yields $1.35\times$ speedup 
and $2.79\times$ on 4 cores compared to single core performance when using the smallest dataset(e.g., x1).

In both dimensions, cache memory and parallelization, which all DBSs have been 
benchmarked, MongoDB seems to perform mainly worse than the rest. This is 
shown that MongoDB as a relationally-treated database seems to be inferior, 
when compared to other traditional relation databases, despite its widespread
popularity. Hence, it seems that when it comes to more
traditional relational databases tasks, such as complex joins and transaction, 
MongoDB is struggling or not compared to PostgreSQL, Vertica and MonetDB. 
This contradiction raises questions about the suitability of MongoDB for certain use cases, 
and highlights the need for further research and evaluation of its performance in comparison 
to other databases. 

For that reason, weblogs query was selected and implemented in native Mongo, using its 
embedded query language~\footnote{Check the new query for MongoDB, in the Github repository}.
Its performance for x32 dataset was 2.4s compared to 22s
with current implementation. Implementing in MongoDB's query language, meant to transform 
all UDFs as part of aggregation pipeline, save the output of that pipeline, in a new
collection and then join the new collection with collection, contained all bad IPs. 
Despite it can be heard difficult, MongoDB documentation and community, are 
extremely fine-grained and the implemention will be straightforward. 


